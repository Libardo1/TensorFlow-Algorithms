{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar algorithm is commonly referred to as the Viterbi algorithm, but also known as a special case of the max-product or min-sum algorithm, which solves the related problem of maximization, or most probable explanation. Instead of attempting to solve the marginal, the goal here is to find the values \\mathbf{x} that maximises the global function (i.e. most probable values in a probabilistic setting), and it can be defined using the arg max:\n",
    "\n",
    "\\operatorname*{arg\\,max}_{\\mathbf{x}} g(\\mathbf{x}).\n",
    "An algorithm that solves this problem is nearly identical to belief propagation, with the sums replaced by maxima in the definitions.[8]\n",
    "\n",
    "It is worth noting that inference problems like marginalization and maximization are NP-hard to solve exactly and approximately (at least for relative error) in a graphical model. More precisely, the marginalization problem defined above is #P-complete and maximization is NP-complete.\n",
    "\n",
    "The memory usage of belief propagation can be reduced through the use of the Island algorithm (at a small cost in time complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dptable(state_prob):\n",
    "    print(\" \".join((\"%10d\" % i) for i in range(state_prob.shape[0])))\n",
    "    for i, prob in enumerate(state_prob.T):\n",
    "        print(\"%.7s: \" % states[i] +\" \".join(\"%.7s\" % (\"%f\" % p) for p in prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenMarkovModel(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Hidden Markov Model Class\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    - S: Number of states.\n",
    "    - T: Transition matrix of size S by S\n",
    "         stores probability from state i to state j.\n",
    "    - E: Emission matrix of size S by N (number of observations)\n",
    "         stores the probability of observing  O_j  from state  S_i. \n",
    "    - T0: Initial state probabilities of size S.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T, E, T0):\n",
    "        # Number of states\n",
    "        self.S = T.shape[0]\n",
    "        \n",
    "        # Emission probability\n",
    "        self.E = tf.constant(E, name='emission_matrix')\n",
    "\n",
    "        # Transition matrix\n",
    "        self.T = tf.constant(T, name='transition_matrix')\n",
    "\n",
    "        # Initial state vector\n",
    "        self.T0 = tf.constant(T0, name='inital_state_vector')\n",
    "\n",
    "    def initialize_path_variables(self, shape):\n",
    "        \n",
    "        pathStates = tf.Variable(tf.zeros(shape, dtype=tf.int64), name='States_matrix')\n",
    "        pathScores = tf.Variable(tf.zeros(shape, dtype=tf.float64), name='Score_matrix')\n",
    "        states_seq = tf.Variable(tf.zeros([shape[0]], dtype=tf.int64), name='States_sequence')\n",
    "        return pathStates, pathScores, states_seq\n",
    "    \n",
    "    def belief_propagation(self, scores):\n",
    "        \n",
    "        scores_reshape = tf.reshape(scores, (-1,1))\n",
    "        return tf.add(scores_reshape, tf.log(self.T))\n",
    "    \n",
    "    def viterbi_inference(self, obs_seq):\n",
    "        \n",
    "        # length of observed sequence\n",
    "        N = len(obs_seq)\n",
    "        \n",
    "        # shape path Variables\n",
    "        shape = [N, self.S]\n",
    "        \n",
    "        # observed sequence\n",
    "        x = tf.constant(obs_seq, name='observation_sequence')\n",
    "        \n",
    "        # Initialize variables\n",
    "        pathStates, pathScores, states_seq = self.initialize_path_variables(shape)       \n",
    "        \n",
    "        # log probability of emission sequence\n",
    "        obs_prob_seq = tf.log(tf.gather(self.E, x))\n",
    "        obs_prob_list = tf.split(0, N, obs_prob_seq)\n",
    "\n",
    "        # step 1 state log-prior update\n",
    "        pathScores = tf.scatter_update(pathScores, 0, tf.log(self.T0) + tf.squeeze(obs_prob_list[0]))\n",
    "            \n",
    "        \n",
    "        for step, obs_prob in enumerate(obs_prob_list[1:]):\n",
    "            \n",
    "            # propagate state belief\n",
    "            belief = self.belief_propagation(pathScores[step, :])\n",
    "\n",
    "            # the inferred state by maximizing global function\n",
    "            # and update state and score matrices \n",
    "            pathStates = tf.scatter_update(pathStates, step + 1, tf.argmax(belief, 0))\n",
    "            pathScores = tf.scatter_update(pathScores, step + 1, tf.reduce_max(belief, 0) + tf.squeeze(obs_prob))\n",
    "\n",
    "        # infer most likely last state\n",
    "        states_seq = tf.scatter_update(states_seq, N-1, tf.argmax(pathScores[N-1, :], 0))\n",
    "        \n",
    "        for step in range(N - 1, 0, -1):\n",
    "            # for every timestep retrieve inferred state\n",
    "            state = states_seq[step]\n",
    "            idx = tf.reshape(tf.pack([step, state]), [1, -1])\n",
    "            state_prob = tf.gather_nd(pathStates, idx)\n",
    "            states_seq = tf.scatter_update(states_seq, step - 1,  state_prob[0])\n",
    "\n",
    "        return states_seq, tf.exp(pathScores) # turn scores back to probabilities\n",
    "    \n",
    "    def run_viterbi(self, obs_seq):\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            state_graph, state_prob_graph = self.viterbi_inference(obs_seq)\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            states_seq, state_prob = sess.run([state_graph, state_prob_graph])\n",
    "\n",
    "        return states_seq, state_prob \n",
    "    \n",
    "    \n",
    "    def _forward(self, y):\n",
    "        # forward pass\n",
    "        # TensorFlow doesn't support indexing. List of Tensors will be used instead\n",
    "        forward = []\n",
    "        \n",
    "        # initialize with state probabilities T0\n",
    "        forward.append(tf.constant(self.T0, shape=(1, self.S), dtype=tf.float64))\n",
    "        #forward.append(tf.ones((1, self.S), dtype=tf.float64) * self.T0)\n",
    "        \n",
    "        # forward belief propagation\n",
    "        for step in range(y.shape[0]):\n",
    "\n",
    "            forward_score = tf.mul(tf.matmul(forward[step], self.T), y[step])\n",
    "            \n",
    "            # normalize scores into probabilities\n",
    "            forward.append(forward_score / tf.reduce_sum(forward_score))\n",
    "\n",
    "        return forward[1:] # remove initial value\n",
    "        \n",
    "\n",
    "    def _backward(self, y):\n",
    "        \n",
    "        N = y.shape[0]\n",
    "        backward = np.zeros((nT + 1, self.K))\n",
    "\n",
    "        # backward pass\n",
    "        backward = [None] * (nT + 1)\n",
    "        backward[-1] = tf.ones((1, self.K), dtype=tf.float64) * (1.0 / self.K)\n",
    "        for t in range(nT, 0, -1):\n",
    "            backward_score = tf.transpose(\n",
    "                tf.matmul(\n",
    "                    tf.matmul(self.P, tf.diag(y[t - 1])),\n",
    "                    tf.transpose(backward[t])\n",
    "                )\n",
    "            )\n",
    "            backward[t - 1] = tmp / tf.reduce_sum(tmp)      \n",
    "        \n",
    "    def forward_backward(self, y):\n",
    "        \"\"\"\n",
    "        runs forward backward algorithm on state probabilities y\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        y : np.array : shape (T, K) where T is number of timesteps and\n",
    "            K is the number of states\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (posterior, forward, backward)\n",
    "        posterior : list of length T of tensorflow graph nodes representing\n",
    "            the posterior probability of each state at each time step\n",
    "        forward : list of length T of tensorflow graph nodes representing\n",
    "            the forward probability of each state at each time step\n",
    "        backward : list of length T of tensorflow graph nodes representing\n",
    "            the backward probability of each state at each time step\n",
    "        \"\"\"\n",
    "\n",
    "        # length of observed sequence\n",
    "        N = len(obs_seq)\n",
    "        \n",
    "        # shape path Variables\n",
    "        shape = [N, self.S]\n",
    "        \n",
    "        # observed sequence\n",
    "        x = tf.constant(obs_seq, name='observation_sequence')\n",
    "        \n",
    "        # Initialize variables\n",
    "        pathStates, pathScores, states_seq = self.initialize_path_variables(shape)       \n",
    "        \n",
    "        # log probability of emission sequence\n",
    "        obs_prob_seq = tf.log(tf.gather(self.E, x))\n",
    "        obs_prob_list = tf.split(0, N, obs_prob_seq)\n",
    "        \n",
    "\n",
    "        posterior = np.zeros((nT, self.K))\n",
    "        \n",
    "        forward = self._forward(y)\n",
    "        \n",
    "        backward = self._backward(y)\n",
    "\n",
    "        # remove initial/final probabilities\n",
    "        forward = forward[1:]\n",
    "        backward = backward[:-1]\n",
    "\n",
    "        # combine and normalize\n",
    "        posterior = [f * b for f, b in zip(forward, backward)]\n",
    "        posterior = [p / tf.reduce_sum(p) for p in posterior]\n",
    "\n",
    "        return posterior, forward, backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p0 = np.array([0.6, 0.4])\n",
    "\n",
    "emi = np.array([[0.5, 0.1],\n",
    "                [0.4, 0.3],\n",
    "                [0.1, 0.6]])\n",
    "\n",
    "trans = np.array([[0.7, 0.3],\n",
    "                  [0.4, 0.6]])\n",
    "\n",
    "states = {0:'Healthy', 1:'Fever'}\n",
    "obs = {0:'normal', 1:'cold', 2:'dizzy'}\n",
    "\n",
    "obs_seq = np.array([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0          1          2\n",
      "Healthy: 0.30000 0.08400 0.00588\n",
      "Fever: 0.04000 0.02700 0.01512\n",
      "\n",
      "Most likely States:  ['normal', 'normal', 'cold']\n"
     ]
    }
   ],
   "source": [
    "model =  HiddenMarkovModel(trans, emi, p0)\n",
    "states_seq, state_prob = model.run_viterbi(obs_seq)\n",
    "dptable(state_prob)\n",
    "print()\n",
    "print(\"Most likely States: \", [obs[s] for s in states_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p0 = np.array([0.5, 0.5])\n",
    "\n",
    "emi = np.array([[0.9, 0.2],\n",
    "                [0.1, 0.8]])\n",
    "\n",
    "trans = np.array([[0.7, 0.3],\n",
    "                  [0.3, 0.7]])\n",
    "\n",
    "states = {0:'rain', 1:'no_rain'}\n",
    "obs = {0:'umbrella', 1:'no_umbrella'}\n",
    "\n",
    "obs_seq = np.array([0, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HiddenMarkovModel' object has no attribute 'K'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d3a4c3778628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mHiddenMarkovModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d0f733058111>\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mnT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HiddenMarkovModel' object has no attribute 'K'"
     ]
    }
   ],
   "source": [
    "model =  HiddenMarkovModel(trans, emi, p0)\n",
    "\n",
    "p, f, b = model.forward_backward(obs_seq)\n",
    "[tf.Session().run(g) for g in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
