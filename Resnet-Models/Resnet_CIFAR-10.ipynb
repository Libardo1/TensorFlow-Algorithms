{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from nesterov import NesterovOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def one_hot_vec(label):\n",
    "    vec = np.zeros(10)\n",
    "    vec[label] = 1\n",
    "    return vec\n",
    "\n",
    "def load_data():\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for i in range (5):\n",
    "        d = unpickle(\"/Volumes/EXTRADRIVE/data/cifar-10-batches-py/data_batch_\" + str(i+1))\n",
    "        x_ = d['data']\n",
    "        y_ = d['labels']\n",
    "        x_all.append(x_)\n",
    "        y_all.append(y_)\n",
    "\n",
    "    d = unpickle('/Volumes/EXTRADRIVE/data/cifar-10-batches-py/test_batch')\n",
    "    x_all.append(d['data'])\n",
    "    y_all.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(x_all) / np.float32(255)\n",
    "    y = np.concatenate(y_all)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "    \n",
    "    pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    x -= pixel_mean\n",
    "\n",
    "    y = map(one_hot_vec, y)\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return (X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def softmax_layer(inpt, shape):\n",
    "    fc_w = weight_variable(shape)\n",
    "    fc_b = tf.Variable(tf.zeros([shape[1]]))\n",
    "\n",
    "    fc_h = tf.nn.softmax(tf.matmul(inpt, fc_w) + fc_b)\n",
    "\n",
    "    return fc_h\n",
    "\n",
    "def conv_layer(inpt, filter_shape, stride):\n",
    "    out_channels = filter_shape[3]\n",
    "\n",
    "    filter_ = weight_variable(filter_shape)\n",
    "    conv = tf.nn.conv2d(inpt, filter=filter_, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "    mean, var = tf.nn.moments(conv, axes=[0,1,2])\n",
    "    beta = tf.Variable(tf.zeros([out_channels]), name=\"beta\")\n",
    "    gamma = weight_variable([out_channels], name=\"gamma\")\n",
    "    \n",
    "    batch_norm = tf.nn.batch_norm_with_global_normalization(\n",
    "        conv, mean, var, beta, gamma, 0.001,\n",
    "        scale_after_normalization=True)\n",
    "\n",
    "    out = tf.nn.relu(batch_norm)\n",
    "\n",
    "    return out\n",
    "\n",
    "def residual_block(inpt, output_depth, down_sample, projection=False):\n",
    "    input_depth = inpt.get_shape().as_list()[3]\n",
    "    if down_sample:\n",
    "        filter_ = [1,2,2,1]\n",
    "        inpt = tf.nn.max_pool(inpt, ksize=filter_, strides=filter_, padding='SAME')\n",
    "\n",
    "    conv1 = conv_layer(inpt, [3, 3, input_depth, output_depth], 1)\n",
    "    conv2 = conv_layer(conv1, [3, 3, output_depth, output_depth], 1)\n",
    "\n",
    "    if input_depth != output_depth:\n",
    "        if projection:\n",
    "            # Option B: Projection shortcut\n",
    "            input_layer = conv_layer(inpt, [1, 1, input_depth, output_depth], 2)\n",
    "        else:\n",
    "            # Option A: Zero-padding\n",
    "            input_layer = tf.pad(inpt, [[0,0], [0,0], [0,0], [0, output_depth - input_depth]])\n",
    "    else:\n",
    "        input_layer = inpt\n",
    "\n",
    "    res = conv2 + input_layer\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dict = {20:1, 32:2, 44:3, 56:4}\n",
    "\n",
    "# ResNet architectures used for CIFAR-10\n",
    "def resnet(inpt, n):\n",
    "    if n < 20 or (n - 20) % 12 != 0:\n",
    "        print \"ResNet depth invalid.\"\n",
    "        return\n",
    "\n",
    "    num_conv = (n - 20) / 12 + 1\n",
    "    layers = []\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_layer(inpt, [3, 3, 3, 16], 1)\n",
    "        layers.append(conv1)\n",
    "\n",
    "    for i in range (num_conv):\n",
    "        with tf.variable_scope('conv2_%d' % (i+1)):\n",
    "            conv2_x = residual_block(layers[-1], 16, False)\n",
    "            conv2 = residual_block(conv2_x, 16, False)\n",
    "            layers.append(conv2_x)\n",
    "            layers.append(conv2)\n",
    "\n",
    "        assert conv2.get_shape().as_list()[1:] == [32, 32, 16]\n",
    "\n",
    "    for i in range (num_conv):\n",
    "        down_sample = True if i == 0 else False\n",
    "        with tf.variable_scope('conv3_%d' % (i+1)):\n",
    "            conv3_x = residual_block(layers[-1], 32, down_sample)\n",
    "            conv3 = residual_block(conv3_x, 32, False)\n",
    "            layers.append(conv3_x)\n",
    "            layers.append(conv3)\n",
    "\n",
    "        assert conv3.get_shape().as_list()[1:] == [16, 16, 32]\n",
    "    \n",
    "    for i in range (num_conv):\n",
    "        down_sample = True if i == 0 else False\n",
    "        with tf.variable_scope('conv4_%d' % (i+1)):\n",
    "            conv4_x = residual_block(layers[-1], 64, down_sample)\n",
    "            conv4 = residual_block(conv4_x, 64, False)\n",
    "            layers.append(conv4_x)\n",
    "            layers.append(conv4)\n",
    "\n",
    "        assert conv4.get_shape().as_list()[1:] == [8, 8, 64]\n",
    "\n",
    "    with tf.variable_scope('fc'):\n",
    "        global_pool = tf.reduce_mean(layers[-1], [1, 2])\n",
    "        assert global_pool.get_shape().as_list()[1:] == [64]\n",
    "        \n",
    "        out = softmax_layer(global_pool, [64, 10])\n",
    "        layers.append(out)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Learning rate')\n",
    "flags.DEFINE_integer('batch_size', 25, 'Batch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "X = tf.placeholder(\"float\", [batch_size, 32, 32, 3])\n",
    "Y = tf.placeholder(\"float\", [batch_size, 10])\n",
    "learning_rate = tf.placeholder(\"float\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find checkpoint to restore from. Starting over.\n",
      "training on image #0\n",
      "training on image #512\n",
      "training on image #1024\n",
      "training on image #1536\n",
      "training on image #2048\n",
      "training on image #2560\n",
      "training on image #3072\n",
      "training on image #3584\n",
      "training on image #4096\n",
      "training on image #4608\n",
      "training on image #5120\n",
      "training on image #5632\n",
      "training on image #6144\n",
      "training on image #6656\n",
      "training on image #7168\n",
      "training on image #7680\n",
      "training on image #8192\n",
      "training on image #8704\n",
      "training on image #9216\n",
      "training on image #9728\n",
      "training on image #0\n",
      "training on image #512\n",
      "training on image #1024\n",
      "training on image #1536\n",
      "training on image #2048\n",
      "training on image #2560\n",
      "training on image #3072\n",
      "training on image #3584\n",
      "training on image #4096\n",
      "training on image #4608\n",
      "training on image #5120\n",
      "training on image #5632\n",
      "training on image #6144\n",
      "training on image #6656\n",
      "training on image #7168\n",
      "training on image #7680\n",
      "training on image #8192\n",
      "training on image #8704\n",
      "training on image #9216\n",
      "training on image #9728\n",
      "[0.3359375]\n",
      "[0.28125]\n",
      "[0.3203125]\n",
      "[0.234375]\n",
      "[0.2734375]\n",
      "[0.2578125]\n",
      "[0.3515625]\n",
      "[0.3203125]\n",
      "[0.28125]\n",
      "[0.328125]\n",
      "[0.3046875]\n",
      "[0.3125]\n",
      "[0.265625]\n",
      "[0.359375]\n",
      "[0.28125]\n",
      "[0.21875]\n",
      "[0.296875]\n",
      "[0.25]\n",
      "[0.3046875]\n",
      "[0.3125]\n",
      "[0.3203125]\n",
      "[0.2578125]\n",
      "[0.265625]\n",
      "[0.3046875]\n",
      "[0.3125]\n",
      "[0.21875]\n",
      "[0.3359375]\n",
      "[0.34375]\n",
      "[0.28125]\n",
      "[0.265625]\n",
      "[0.3125]\n",
      "[0.2890625]\n",
      "[0.3515625]\n",
      "[0.2578125]\n",
      "[0.2734375]\n",
      "[0.28125]\n",
      "[0.2109375]\n",
      "[0.28125]\n",
      "[0.3203125]\n",
      "[0.34375]\n"
     ]
    }
   ],
   "source": [
    "# ResNet Models\n",
    "net = resnet(X, 20)\n",
    "# net = resnet(X, 32)\n",
    "# net = resnet(X, 44)\n",
    "# net = resnet(X, 56)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(Y*tf.log(net))\n",
    "opt = NesterovOptimizer(learning_rate, 0.9)\n",
    "train_op = opt.minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#checkpoint = tf.train.latest_checkpoint(\".\")\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    print \"Restoring from checkpoint\", checkpoint\n",
    "    saver.restore(sess, checkpoint)\n",
    "else:\n",
    "    print \"Couldn't find checkpoint to restore from. Starting over.\"\n",
    "\n",
    "#for j in range (10):\n",
    "for j in range (2):\n",
    "    #for i in range (0, 50000, batch_size):\n",
    "    for i in range (0, 10000, batch_size):\n",
    "        feed_dict={\n",
    "            X: X_train[i:i + batch_size], \n",
    "            Y: Y_train[i:i + batch_size],\n",
    "            learning_rate: 0.001}\n",
    "        sess.run([train_op], feed_dict=feed_dict)\n",
    "        if i % 512 == 0:\n",
    "            print \"training on image #%d\" % i\n",
    "            saver.save(sess, 'progress', global_step=i)\n",
    "\n",
    "#for i in range (0, 10000, batch_size):\n",
    "for i in range (0, 5000, batch_size):\n",
    "    if i + batch_size < 10000:\n",
    "        acc = sess.run([accuracy],feed_dict={\n",
    "            X: X_test[i:i+batch_size],\n",
    "            Y: Y_test[i:i+batch_size]\n",
    "        })\n",
    "        accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "        print acc\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nesterov.NesterovOptimizer at 0x10c298690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NesterovOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
