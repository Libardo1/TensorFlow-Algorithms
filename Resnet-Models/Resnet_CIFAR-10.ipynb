{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Networks with CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from nesterov import NesterovOptimizer\n",
    "import time\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def one_hot_vec(label):\n",
    "    vec = np.zeros(10)\n",
    "    vec[label] = 1\n",
    "    return vec\n",
    "\n",
    "def load_data():\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for i in range (5):\n",
    "        d = unpickle(\"/Volumes/EXTRADRIVE/data/cifar-10-batches-py/data_batch_\" + str(i+1))\n",
    "        x_ = d['data']\n",
    "        y_ = d['labels']\n",
    "        x_all.append(x_)\n",
    "        y_all.append(y_)\n",
    "\n",
    "    d = unpickle('/Volumes/EXTRADRIVE/data/cifar-10-batches-py/test_batch')\n",
    "    x_all.append(d['data'])\n",
    "    y_all.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(x_all) / np.float32(255)\n",
    "    y = np.concatenate(y_all)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "    \n",
    "    pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    x -= pixel_mean\n",
    "\n",
    "    y = map(one_hot_vec, y)\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return (X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def softmax_layer(inpt, shape):\n",
    "    fc_w = weight_variable(shape)\n",
    "    fc_b = tf.Variable(tf.zeros([shape[1]]))\n",
    "\n",
    "    fc_h = tf.nn.softmax(tf.matmul(inpt, fc_w) + fc_b)\n",
    "\n",
    "    return fc_h\n",
    "\n",
    "def conv_layer(inpt, filter_shape, stride):\n",
    "    out_channels = filter_shape[3]\n",
    "\n",
    "    filter_ = weight_variable(filter_shape)\n",
    "    conv = tf.nn.conv2d(inpt, filter=filter_, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "    mean, var = tf.nn.moments(conv, axes=[0,1,2])\n",
    "    beta = tf.Variable(tf.zeros([out_channels]), name=\"beta\")\n",
    "    gamma = weight_variable([out_channels], name=\"gamma\")\n",
    "    \n",
    "    batch_norm = tf.nn.batch_norm_with_global_normalization(\n",
    "        conv, mean, var, beta, gamma, 0.001,\n",
    "        scale_after_normalization=True)\n",
    "\n",
    "    out = tf.nn.relu(batch_norm)\n",
    "\n",
    "    return out\n",
    "\n",
    "def residual_block(inpt, output_depth, down_sample, projection=False):\n",
    "    input_depth = inpt.get_shape().as_list()[3]\n",
    "    if down_sample:\n",
    "        filter_ = [1,2,2,1]\n",
    "        inpt = tf.nn.max_pool(inpt, ksize=filter_, strides=filter_, padding='SAME')\n",
    "\n",
    "    conv1 = conv_layer(inpt, [3, 3, input_depth, output_depth], 1)\n",
    "    conv2 = conv_layer(conv1, [3, 3, output_depth, output_depth], 1)\n",
    "\n",
    "    if input_depth != output_depth:\n",
    "        if projection:\n",
    "            # Option B: Projection shortcut\n",
    "            input_layer = conv_layer(inpt, [1, 1, input_depth, output_depth], 2)\n",
    "        else:\n",
    "            # Option A: Zero-padding\n",
    "            input_layer = tf.pad(inpt, [[0,0], [0,0], [0,0], [0, output_depth - input_depth]])\n",
    "    else:\n",
    "        input_layer = inpt\n",
    "\n",
    "    res = conv2 + input_layer\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet architectures used for CIFAR-10\n",
    "def resnet(inpt, n):\n",
    "    if n < 20 or (n - 20) % 12 != 0:\n",
    "        print(\"ResNet depth invalid.\")\n",
    "        return\n",
    "\n",
    "    num_conv = int((n - 20) / 12 + 1)\n",
    "    layers = []\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_layer(inpt, [3, 3, 3, 16], 1)\n",
    "        layers.append(conv1)\n",
    "\n",
    "    for i in range (num_conv):\n",
    "        with tf.variable_scope('conv2_%d' % (i+1)):\n",
    "            conv2_x = residual_block(layers[-1], 16, False)\n",
    "            conv2 = residual_block(conv2_x, 16, False)\n",
    "            layers.append(conv2_x)\n",
    "            layers.append(conv2)\n",
    "\n",
    "        assert conv2.get_shape().as_list()[1:] == [32, 32, 16]\n",
    "\n",
    "    for i in range (num_conv):\n",
    "        down_sample = True if i == 0 else False\n",
    "        with tf.variable_scope('conv3_%d' % (i+1)):\n",
    "            conv3_x = residual_block(layers[-1], 32, down_sample)\n",
    "            conv3 = residual_block(conv3_x, 32, False)\n",
    "            layers.append(conv3_x)\n",
    "            layers.append(conv3)\n",
    "\n",
    "        assert conv3.get_shape().as_list()[1:] == [16, 16, 32]\n",
    "    \n",
    "    for i in range (num_conv):\n",
    "        down_sample = True if i == 0 else False\n",
    "        with tf.variable_scope('conv4_%d' % (i+1)):\n",
    "            conv4_x = residual_block(layers[-1], 64, down_sample)\n",
    "            conv4 = residual_block(conv4_x, 64, False)\n",
    "            layers.append(conv4_x)\n",
    "            layers.append(conv4)\n",
    "\n",
    "        assert conv4.get_shape().as_list()[1:] == [8, 8, 64]\n",
    "\n",
    "    with tf.variable_scope('fc'):\n",
    "        global_pool = tf.reduce_mean(layers[-1], [1, 2])\n",
    "        assert global_pool.get_shape().as_list()[1:] == [64]\n",
    "        \n",
    "        out = softmax_layer(global_pool, [64, 10])\n",
    "        layers.append(out)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(n):\n",
    "      \n",
    "    X = tf.placeholder(\"float\", [FLAGS.batch_size, 32, 32, 3])\n",
    "    Y = tf.placeholder(\"float\", [FLAGS.batch_size, 10])\n",
    "\n",
    "    # ResNet Models\n",
    "    net = resnet(X, n)\n",
    "\n",
    "    # cross entropy loss\n",
    "    loss = -tf.reduce_mean(Y*tf.log(net))\n",
    "\n",
    "    # Optimize\n",
    "    #opt = NesterovOptimizer(FLAGS.learning_rate, 0.9)\n",
    "    opt =tf.train.MomentumOptimizer(FLAGS.learning_rate, 0.9)\n",
    "    train_op = opt.minimize(loss)\n",
    "\n",
    "    # predict\n",
    "    correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    #summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    print('Start training...\\n')\n",
    "    train_acc = []\n",
    "    \n",
    "    for epoch in xrange (FLAGS.epoch):\n",
    "        print('Epoch %d \\n' % (epoch))\n",
    "        for i in xrange (0, FLAGS.sample, FLAGS.batch_size):\n",
    "\n",
    "            start_time = time.time()\n",
    "            feed_dict={\n",
    "                X: X_train[i:i + FLAGS.batch_size], \n",
    "                Y: Y_train[i:i + FLAGS.batch_size]}\n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                                         feed_dict=feed_dict)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Write the summaries and print an overview fairly often.\n",
    "            #if i % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={\n",
    "                X: X_train[i:i + FLAGS.batch_size],\n",
    "                Y: Y_train[i:i + FLAGS.batch_size]\n",
    "            })\n",
    "            step = int(i / FLAGS.batch_size)\n",
    "            print('Step %d: loss = %.3f Accuracy = %.3f (%.3f sec)' % (step, loss_value, acc, duration))\n",
    "            train_acc.append(acc)\n",
    "            #accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "            # Update the events file.\n",
    "            #summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "            #summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "    \n",
    "    print('Computing Test Accuracy ...\\n')\n",
    "    accuracy = []\n",
    "    \n",
    "    for i in range (0, FLAGS.sample_test, FLAGS.batch_size):\n",
    "        if i + FLAGS.batch_size < FLAGS.sample_test:\n",
    "\n",
    "            acc = sess.run(accuracy,feed_dict={\n",
    "                X: X_test[i:i + FLAGS.batch_size],\n",
    "                Y: Y_test[i:i + FLAGS.batch_size]\n",
    "            })\n",
    "        accuracy_list.append(acc)\n",
    "    \n",
    "    test_accuracy = np.mean(np.array(accuracy_list))\n",
    "    print(\"Test Accuracy: %.3f\" % (test_accuracy))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Learning rate')\n",
    "flags.DEFINE_integer('batch_size', 128, 'Batch size')\n",
    "flags.DEFINE_integer('epoch', 3, 'Number of epochs')\n",
    "flags.DEFINE_integer('sample', 128*30, 'Number of samples in trainset') # Full train data 50000\n",
    "flags.DEFINE_integer('sample_test', 128*10, 'Number of samples in testset') # Full test data 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Load data...\\n')\n",
    "X_train, Y_train, X_test, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch 0 \n",
      "\n",
      "Step 0: loss = 0.230 Accuracy = 0.125 (2.900 sec)\n",
      "Step 1: loss = 0.231 Accuracy = 0.094 (2.905 sec)\n",
      "Step 2: loss = 0.230 Accuracy = 0.094 (2.384 sec)\n",
      "Step 3: loss = 0.230 Accuracy = 0.094 (2.386 sec)\n",
      "Step 4: loss = 0.231 Accuracy = 0.062 (2.989 sec)\n",
      "Step 5: loss = 0.232 Accuracy = 0.062 (2.827 sec)\n",
      "Step 6: loss = 0.229 Accuracy = 0.180 (2.297 sec)\n",
      "Step 7: loss = 0.231 Accuracy = 0.094 (2.398 sec)\n",
      "Step 8: loss = 0.231 Accuracy = 0.164 (3.037 sec)\n",
      "Step 9: loss = 0.231 Accuracy = 0.086 (2.335 sec)\n",
      "Step 10: loss = 0.230 Accuracy = 0.094 (2.357 sec)\n",
      "Step 11: loss = 0.230 Accuracy = 0.078 (2.439 sec)\n",
      "Step 12: loss = 0.231 Accuracy = 0.070 (2.251 sec)\n",
      "Step 13: loss = 0.230 Accuracy = 0.109 (2.298 sec)\n",
      "Step 14: loss = 0.230 Accuracy = 0.086 (2.338 sec)\n",
      "Step 15: loss = 0.231 Accuracy = 0.070 (2.368 sec)\n",
      "Step 16: loss = 0.230 Accuracy = 0.148 (3.073 sec)\n",
      "Step 17: loss = 0.231 Accuracy = 0.062 (2.852 sec)\n",
      "Step 18: loss = 0.230 Accuracy = 0.133 (2.384 sec)\n",
      "Step 19: loss = 0.230 Accuracy = 0.062 (2.637 sec)\n",
      "Step 20: loss = 0.231 Accuracy = 0.102 (2.598 sec)\n",
      "Step 21: loss = 0.230 Accuracy = 0.125 (2.426 sec)\n",
      "Step 22: loss = 0.230 Accuracy = 0.070 (2.371 sec)\n",
      "Step 23: loss = 0.232 Accuracy = 0.094 (2.634 sec)\n",
      "Step 24: loss = 0.231 Accuracy = 0.141 (2.854 sec)\n",
      "Step 25: loss = 0.230 Accuracy = 0.094 (2.644 sec)\n",
      "Step 26: loss = 0.230 Accuracy = 0.078 (2.971 sec)\n",
      "Step 27: loss = 0.231 Accuracy = 0.109 (2.952 sec)\n",
      "Step 28: loss = 0.231 Accuracy = 0.125 (3.148 sec)\n",
      "Step 29: loss = 0.231 Accuracy = 0.125 (2.566 sec)\n",
      "Epoch 1 \n",
      "\n",
      "Step 0: loss = 0.230 Accuracy = 0.125 (2.519 sec)\n",
      "Step 1: loss = 0.231 Accuracy = 0.086 (2.809 sec)\n",
      "Step 2: loss = 0.230 Accuracy = 0.102 (2.770 sec)\n",
      "Step 3: loss = 0.230 Accuracy = 0.094 (2.454 sec)\n",
      "Step 4: loss = 0.231 Accuracy = 0.062 (2.411 sec)\n",
      "Step 5: loss = 0.232 Accuracy = 0.062 (2.934 sec)\n",
      "Step 6: loss = 0.229 Accuracy = 0.172 (2.704 sec)\n",
      "Step 7: loss = 0.231 Accuracy = 0.094 (2.691 sec)\n",
      "Step 8: loss = 0.231 Accuracy = 0.164 (2.536 sec)\n",
      "Step 9: loss = 0.231 Accuracy = 0.086 (2.510 sec)\n",
      "Step 10: loss = 0.230 Accuracy = 0.094 (2.281 sec)\n",
      "Step 11: loss = 0.230 Accuracy = 0.070 (2.257 sec)\n",
      "Step 12: loss = 0.231 Accuracy = 0.070 (2.351 sec)\n",
      "Step 13: loss = 0.230 Accuracy = 0.117 (2.313 sec)\n",
      "Step 14: loss = 0.230 Accuracy = 0.086 (2.262 sec)\n",
      "Step 15: loss = 0.231 Accuracy = 0.070 (2.290 sec)\n",
      "Step 16: loss = 0.230 Accuracy = 0.148 (2.306 sec)\n",
      "Step 17: loss = 0.231 Accuracy = 0.062 (2.468 sec)\n",
      "Step 18: loss = 0.230 Accuracy = 0.133 (2.908 sec)\n",
      "Step 19: loss = 0.230 Accuracy = 0.062 (2.569 sec)\n",
      "Step 20: loss = 0.231 Accuracy = 0.102 (2.736 sec)\n",
      "Step 21: loss = 0.230 Accuracy = 0.125 (3.112 sec)\n",
      "Step 22: loss = 0.230 Accuracy = 0.070 (2.809 sec)\n",
      "Step 23: loss = 0.232 Accuracy = 0.102 (2.650 sec)\n",
      "Step 24: loss = 0.231 Accuracy = 0.148 (2.858 sec)\n",
      "Step 25: loss = 0.230 Accuracy = 0.109 (2.931 sec)\n",
      "Step 26: loss = 0.230 Accuracy = 0.078 (3.264 sec)\n",
      "Step 27: loss = 0.230 Accuracy = 0.117 (2.684 sec)\n",
      "Step 28: loss = 0.231 Accuracy = 0.102 (2.918 sec)\n",
      "Step 29: loss = 0.230 Accuracy = 0.125 (2.568 sec)\n",
      "Epoch 2 \n",
      "\n",
      "Step 0: loss = 0.230 Accuracy = 0.125 (2.466 sec)\n",
      "Step 1: loss = 0.231 Accuracy = 0.094 (2.384 sec)\n",
      "Step 2: loss = 0.230 Accuracy = 0.102 (2.418 sec)\n",
      "Step 3: loss = 0.230 Accuracy = 0.102 (2.412 sec)\n",
      "Step 4: loss = 0.231 Accuracy = 0.070 (2.331 sec)\n",
      "Step 5: loss = 0.232 Accuracy = 0.062 (2.317 sec)\n",
      "Step 6: loss = 0.229 Accuracy = 0.180 (3.640 sec)\n",
      "Step 7: loss = 0.231 Accuracy = 0.094 (2.862 sec)\n",
      "Step 8: loss = 0.231 Accuracy = 0.148 (2.597 sec)\n",
      "Step 9: loss = 0.231 Accuracy = 0.086 (2.829 sec)\n",
      "Step 10: loss = 0.230 Accuracy = 0.094 (2.994 sec)\n",
      "Step 11: loss = 0.230 Accuracy = 0.078 (2.703 sec)\n",
      "Step 12: loss = 0.231 Accuracy = 0.070 (2.301 sec)\n",
      "Step 13: loss = 0.230 Accuracy = 0.125 (2.368 sec)\n",
      "Step 14: loss = 0.230 Accuracy = 0.094 (2.382 sec)\n",
      "Step 15: loss = 0.231 Accuracy = 0.070 (2.514 sec)\n",
      "Step 16: loss = 0.230 Accuracy = 0.141 (2.934 sec)\n",
      "Step 17: loss = 0.231 Accuracy = 0.070 (3.219 sec)\n",
      "Step 18: loss = 0.230 Accuracy = 0.141 (3.037 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-030f186d0fc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 32, 44, 56\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-297d41ed9ee7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     49\u001b[0m             acc = sess.run(accuracy,feed_dict={\n\u001b[1;32m     50\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             })\n\u001b[1;32m     53\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marvinbertin/anaconda/envs/TensorFlow-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marvinbertin/anaconda/envs/TensorFlow-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 564\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marvinbertin/anaconda/envs/TensorFlow-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 637\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/marvinbertin/anaconda/envs/TensorFlow-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marvinbertin/anaconda/envs/TensorFlow-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m--> 628\u001b[0;31m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = train_model(20) # 32, 44, 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
