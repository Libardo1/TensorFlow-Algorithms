{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Networks with CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from nesterov import NesterovOptimizer\n",
    "import time\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def one_hot_vec(label):\n",
    "    vec = np.zeros(10)\n",
    "    vec[label] = 1\n",
    "    return vec\n",
    "\n",
    "def load_data():\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for i in range (5):\n",
    "        d = unpickle(\"/Volumes/EXTRADRIVE/data/cifar-10-batches-py/data_batch_\" + str(i+1))\n",
    "        x_ = d['data']\n",
    "        y_ = d['labels']\n",
    "        x_all.append(x_)\n",
    "        y_all.append(y_)\n",
    "\n",
    "    d = unpickle('/Volumes/EXTRADRIVE/data/cifar-10-batches-py/test_batch')\n",
    "    x_all.append(d['data'])\n",
    "    y_all.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(x_all) / np.float32(255)\n",
    "    y = np.concatenate(y_all)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "    \n",
    "    pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    x -= pixel_mean\n",
    "\n",
    "    y = map(one_hot_vec, y)\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return (X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def softmax_layer(inpt, shape):\n",
    "    fc_w = weight_variable(shape)\n",
    "    fc_b = tf.Variable(tf.zeros([shape[1]]))\n",
    "\n",
    "    fc_h = tf.nn.softmax(tf.matmul(inpt, fc_w) + fc_b)\n",
    "\n",
    "    return fc_h\n",
    "\n",
    "def conv_layer(inpt, filter_shape, stride):\n",
    "    out_channels = filter_shape[3]\n",
    "\n",
    "    filter_ = weight_variable(filter_shape)\n",
    "    conv = tf.nn.conv2d(inpt, filter=filter_, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "    mean, var = tf.nn.moments(conv, axes=[0,1,2])\n",
    "    beta = tf.Variable(tf.zeros([out_channels]), name=\"beta\")\n",
    "    gamma = weight_variable([out_channels], name=\"gamma\")\n",
    "    \n",
    "    batch_norm = tf.nn.batch_norm_with_global_normalization(\n",
    "        conv, mean, var, beta, gamma, 0.001,\n",
    "        scale_after_normalization=True)\n",
    "\n",
    "    out = tf.nn.relu(batch_norm)\n",
    "\n",
    "    return out\n",
    "\n",
    "def residual_block(inpt, output_depth, down_sample, projection=False):\n",
    "    input_depth = inpt.get_shape().as_list()[3]\n",
    "    if down_sample:\n",
    "        filter_ = [1,2,2,1]\n",
    "        inpt = tf.nn.max_pool(inpt, ksize=filter_, strides=filter_, padding='SAME')\n",
    "\n",
    "    conv1 = conv_layer(inpt, [3, 3, input_depth, output_depth], 1)\n",
    "    conv2 = conv_layer(conv1, [3, 3, output_depth, output_depth], 1)\n",
    "\n",
    "    if input_depth != output_depth:\n",
    "        if projection:\n",
    "            # Option B: Projection shortcut\n",
    "            input_layer = conv_layer(inpt, [1, 1, input_depth, output_depth], 2)\n",
    "        else:\n",
    "            # Option A: Zero-padding\n",
    "            input_layer = tf.pad(inpt, [[0,0], [0,0], [0,0], [0, output_depth - input_depth]])\n",
    "    else:\n",
    "        input_layer = inpt\n",
    "\n",
    "    res = conv2 + input_layer\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet architectures used for CIFAR-10\n",
    "def resnet(inpt, n):\n",
    "    if n < 20 or (n - 20) % 12 != 0:\n",
    "        print(\"ResNet depth invalid.\")\n",
    "        return\n",
    "\n",
    "    num_conv = int((n - 20) / 12 + 1)\n",
    "    layers = []\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_layer(inpt, [3, 3, 3, 16], 1)\n",
    "        layers.append(conv1)\n",
    "\n",
    "    for i in range (num_conv):\n",
    "        with tf.variable_scope('conv2_%d' % (i+1)):\n",
    "            conv2_x = residual_block(layers[-1], 16, False)\n",
    "            conv2 = residual_block(conv2_x, 16, False)\n",
    "            layers.append(conv2_x)\n",
    "            layers.append(conv2)\n",
    "\n",
    "        assert conv2.get_shape().as_list()[1:] == [32, 32, 16]\n",
    "\n",
    "    for i in range (num_conv):\n",
    "        down_sample = True if i == 0 else False\n",
    "        with tf.variable_scope('conv3_%d' % (i+1)):\n",
    "            conv3_x = residual_block(layers[-1], 32, down_sample)\n",
    "            conv3 = residual_block(conv3_x, 32, False)\n",
    "            layers.append(conv3_x)\n",
    "            layers.append(conv3)\n",
    "\n",
    "        assert conv3.get_shape().as_list()[1:] == [16, 16, 32]\n",
    "    \n",
    "    for i in range (num_conv):\n",
    "        down_sample = True if i == 0 else False\n",
    "        with tf.variable_scope('conv4_%d' % (i+1)):\n",
    "            conv4_x = residual_block(layers[-1], 64, down_sample)\n",
    "            conv4 = residual_block(conv4_x, 64, False)\n",
    "            layers.append(conv4_x)\n",
    "            layers.append(conv4)\n",
    "\n",
    "        assert conv4.get_shape().as_list()[1:] == [8, 8, 64]\n",
    "\n",
    "    with tf.variable_scope('fc'):\n",
    "        global_pool = tf.reduce_mean(layers[-1], [1, 2])\n",
    "        assert global_pool.get_shape().as_list()[1:] == [64]\n",
    "        \n",
    "        out = softmax_layer(global_pool, [64, 10])\n",
    "        layers.append(out)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(n):\n",
    "      \n",
    "    X = tf.placeholder(\"float\", [FLAGS.batch_size, 32, 32, 3])\n",
    "    Y = tf.placeholder(\"float\", [FLAGS.batch_size, 10])\n",
    "\n",
    "    # ResNet Models\n",
    "    net = resnet(X, n)\n",
    "\n",
    "    # cross entropy loss\n",
    "    loss = -tf.reduce_mean(Y*tf.log(net))\n",
    "\n",
    "    # Optimize\n",
    "    #opt = NesterovOptimizer(FLAGS.learning_rate, 0.9)\n",
    "    opt =tf.train.MomentumOptimizer(FLAGS.learning_rate, 0.9)\n",
    "    train_op = opt.minimize(loss)\n",
    "\n",
    "    # predict\n",
    "    correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    #summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    print('Start training...\\n')\n",
    "    train_acc = []\n",
    "    \n",
    "    for epoch in xrange (FLAGS.epoch):\n",
    "        print('Epoch %d \\n' % (epoch))\n",
    "        for i in xrange (0, FLAGS.sample, FLAGS.batch_size):\n",
    "\n",
    "            start_time = time.time()\n",
    "            feed_dict={\n",
    "                X: X_train[i:i + FLAGS.batch_size], \n",
    "                Y: Y_train[i:i + FLAGS.batch_size]}\n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                                         feed_dict=feed_dict)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Write the summaries and print an overview fairly often.\n",
    "            #if i % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={\n",
    "                X: X_train[i:i + FLAGS.batch_size],\n",
    "                Y: Y_train[i:i + FLAGS.batch_size]\n",
    "            })\n",
    "            step = int(i / FLAGS.batch_size)\n",
    "            print('Step %d: loss = %.3f Accuracy = %.3f (%.3f sec)' % (step, loss_value, acc, duration))\n",
    "            train_acc.append(acc)\n",
    "            #accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "            # Update the events file.\n",
    "            #summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "            #summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "    \n",
    "    print('Computing Test Accuracy ...\\n')\n",
    "    accuracy = []\n",
    "    \n",
    "    for i in range (0, FLAGS.sample_test, FLAGS.batch_size):\n",
    "        if i + FLAGS.batch_size < FLAGS.sample_test:\n",
    "\n",
    "            acc = sess.run(accuracy,feed_dict={\n",
    "                X: X_test[i:i + FLAGS.batch_size],\n",
    "                Y: Y_test[i:i + FLAGS.batch_size]\n",
    "            })\n",
    "        accuracy_list.append(acc)\n",
    "    \n",
    "    test_accuracy = np.mean(np.array(accuracy_list))\n",
    "    print(\"Test Accuracy: %.3f\" % (test_accuracy))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Learning rate')\n",
    "flags.DEFINE_integer('batch_size', 128, 'Batch size')\n",
    "flags.DEFINE_integer('epoch', 3, 'Number of epochs')\n",
    "flags.DEFINE_integer('sample', 128*30, 'Number of samples in trainset') # Full train data 50000\n",
    "flags.DEFINE_integer('sample_test', 128*10, 'Number of samples in testset') # Full test data 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Load data...\\n')\n",
    "X_train, Y_train, X_test, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch 0 \n",
      "\n",
      "Step 0: loss = 0.229 Accuracy = 0.156 (2.624 sec)\n",
      "Step 1: loss = 0.229 Accuracy = 0.133 (2.930 sec)\n",
      "Step 2: loss = 0.230 Accuracy = 0.125 (3.091 sec)\n",
      "Step 3: loss = 0.231 Accuracy = 0.102 (2.609 sec)\n",
      "Step 4: loss = 0.230 Accuracy = 0.102 (3.133 sec)\n"
     ]
    }
   ],
   "source": [
    "acc = train_model(20) # 32, 44, 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
